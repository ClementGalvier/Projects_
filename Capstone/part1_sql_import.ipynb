{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Part 1 - Aggregating data using SQLite and pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part consist in uploading all datasets into 1 database, to join, groupby and get our working dataset ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use sqlite3 for its easy API, but also its friendly UI which enables to track changes and query the results very easily to ensure data quality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # pandas for data manipulation\n",
    "import csv, sqlite3 # sqlite for sqlite local client interraction\n",
    "import aws_creds # to store my AWS credentials without displaying in the notebook\n",
    "\n",
    "import boto3 # for AWS\n",
    "from io import StringIO # python3; python2: BytesIO \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files shown below were downloaded from<br>\n",
    "*https://www.data.gouv.fr/fr/datasets/bases-de-donnees-annuelles-des-accidents-corporels-de-la-circulation-routiere-annees-de-2005-a-2021/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing each file in pandas before injecting them in the database using SQL\n",
    "#Locations file\n",
    "loc_2019 = pd.read_csv('./source_data_locations/locations_2019.csv', sep=';')\n",
    "loc_2020 = pd.read_csv('./source_data_locations/locations_2020.csv', sep=';')\n",
    "loc_2021 = pd.read_csv('./source_data_locations/locations_2021.csv', sep=';')\n",
    "\n",
    "#Users files\n",
    "ppl_2019 = pd.read_csv('./source_data_people/people_involved_2019.csv', sep=';')\n",
    "ppl_2020 = pd.read_csv('./source_data_people/people_involved_2020.csv', sep=';')\n",
    "ppl_2021 = pd.read_csv('./source_data_people/people_involved_2021.csv', sep=';')\n",
    "\n",
    "\n",
    "#Specificities\n",
    "specs_2019 = pd.read_csv('./source_data_specs/specificities_2019.csv', sep=';')\n",
    "specs_2020 = pd.read_csv('./source_data_specs/specificities_2020.csv', sep=';')\n",
    "specs_2021 = pd.read_csv('./source_data_specs/specificities_2021.csv', sep=';')\n",
    "\n",
    "\n",
    "#vehicules\n",
    "vehi_2019 = pd.read_csv('./source_data_vehicules/vehicules_2019.csv', sep=';')\n",
    "vehi_2020 = pd.read_csv('./source_data_vehicules/vehicules_2020.csv', sep=';')\n",
    "vehi_2021 = pd.read_csv('./source_data_vehicules/vehicules_2021.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Establishing a connection with SQLite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect('db/capstone_sqlite.db') # creating a brand new database in sqlite to store our tables and schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = con.cursor() #our cursor to interact with the database (to query it, but also INSERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97315"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing our  tables into newly created sqlite database\n",
    "#locations\n",
    "loc_2019.to_sql('tb_locations_2019', con) \n",
    "loc_2020.to_sql('tb_locations_2020', con) \n",
    "loc_2021.to_sql('tb_locations_2021', con) \n",
    "\n",
    "#people\n",
    "ppl_2019.to_sql('people_involved_2019', con)\n",
    "ppl_2020.to_sql('people_involved_2020', con)\n",
    "ppl_2021.to_sql('people_involved_2021', con)\n",
    "\n",
    "\n",
    "#Specificities\n",
    "specs_2019.to_sql('specificities_2019', con)\n",
    "specs_2020.to_sql('specificities_2020', con)\n",
    "specs_2021.to_sql('specificities_2021', con)\n",
    "\n",
    "#vehicules\n",
    "vehi_2019.to_sql('vehicules_2019', con)\n",
    "vehi_2020.to_sql('vehicules_2020', con)\n",
    "vehi_2021.to_sql('vehicules_2021', con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Appending data to the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SQL script hidden here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TABLE ppl AS\n",
    "#   SELECT *\n",
    "#   FROM people_involved_2019;  --all columns in people 2019,2020 and 2021 are the same\n",
    "  \n",
    "# CREATE TABLE specs AS\n",
    "#   SELECT *\n",
    "#   FROM specificities_2019; --all columns in specificities 2019,2020 and 2021 are the same\n",
    "\n",
    "# CREATE TABLE vehi AS\n",
    "#   SELECT *\n",
    "#   FROM vehicules_2019 ; --all columns in vehicules 2019,2020 and 2021 are the same\n",
    "\n",
    "\n",
    "# CREATE TABLE loc AS\n",
    "#   SELECT *\n",
    "#   FROM tb_locations_2019 ; --all columns in locations 2019,2020 and 2021 are the same\n",
    "\n",
    "\n",
    "# --consolidating specs tables\n",
    "# INSERT INTO specs SELECT * FROM specificities_2019;\n",
    "# INSERT INTO specs SELECT * FROM specificities_2020;\n",
    "# INSERT INTO specs SELECT * FROM specificities_2021;\n",
    "\n",
    "# --consolidatings people_involved tables\n",
    "# INSERT INTO ppl SELECT * FROM people_involved_2019;\n",
    "# INSERT INTO ppl SELECT * FROM people_involved_2020;\n",
    "# INSERT INTO ppl SELECT * FROM people_involved_2021;\n",
    "\n",
    "# --consolidatings vehicule tables\n",
    "# INSERT INTO vehi SELECT * FROM vehicules_2019;\n",
    "# INSERT INTO vehi SELECT * FROM vehicules_2020;\n",
    "# INSERT INTO vehi SELECT * FROM vehicules_2021;\n",
    "\n",
    "# --consolidatings locations tables\n",
    "# INSERT INTO loc SELECT * FROM tb_locations_2019;\n",
    "# INSERT INTO loc SELECT * FROM tb_locations_2020;\n",
    "# INSERT INTO loc SELECT * FROM tb_locations_2021;\n",
    "\n",
    "\n",
    "# CREATE TABLE df_sql ('Num_Acc', 'id_vehicule', 'place', 'catu', 'grav', 'sexe', 'an_nais', \n",
    "#        'trajet', 'secu1', 'secu2', 'secu3', 'locp', 'actp', 'etatp', 'senc', \n",
    "#        'catv', 'obs', 'obsm', 'choc', 'manv', 'motor', 'occutc', 'catr',\n",
    "#        'voie', 'v1', 'v2', 'circ', 'nbv', 'vosp', 'prof', 'pr', 'pr1', 'plan', \n",
    "#        'lartpc', 'larrout', 'surf', 'infra', 'situ', 'vma', 'jour', 'mois', \n",
    "#        'an', 'hrmn', 'lum', 'dep', 'com', 'agg', 'int', 'atm', 'col', 'adr',\n",
    "#        'lat', 'long');\n",
    "       \n",
    "# INSERT INTO df_sql \n",
    "#     SELECT \n",
    "#         p.Num_Acc, p.id_vehicule, p.place, p.catu, p.grav, p.sexe, p.an_nais,p.trajet, p.secu1, p.secu2, p.secu3, p.locp, p.actp, p.etatp, \n",
    "#         v.senc, v.catv, v.obs, v.obsm, v.choc, v.manv, v.motor, v.occutc, \n",
    "#         l.catr,l.voie, l.v1, l.v2, l.circ, l.nbv, l.vosp, l.prof, l.pr, l.pr1, l.plan, l.lartpc, l.larrout, l.surf, l.infra, l.situ, l.vma, \n",
    "#         s.jour, s.mois,s.an, s.hrmn, s.lum, s.dep, s.com, s.agg, s.int, s.atm, s.col, s.adr,s.lat, s.long\n",
    "#     FROM ppl p JOIN vehi v ON p.Num_Acc = v.Num_Acc AND p.id_vehicule = v.id_vehicule JOIN loc l ON p.Num_Acc = l.Num_Acc JOIN specs s ON p.Num_Acc = s.Num_Acc\n",
    "#     GROUP BY p.Num_Acc, p.id_vehicule, p.place, p.catu, p.grav, p.sexe, p.an_nais,p.trajet, p.secu1, p.secu2, p.secu3, p.locp, p.actp, p.etatp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Python code calling the script stored in a separate file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#script to CREATE TABLES with each df headers. It also INSERT data, and multi JOIN them together\n",
    "with open('db/sql_script_30112022.sql', 'r') as sql_file:\n",
    "    sql_script = sql_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 23.4 s\n",
      "Wall time: 37 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1d9014342d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cur.executescript(sql_script) #executing the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table data after updation :\n",
      "[(201900000001, '138\\xa0306\\xa0524', 1, 1, 4, 2, 1993, 5, 1, 0, -1, -1, ' -1', -1, 2, 7, 0, 2, 5, 23, 1, None, 1, '3', 0.0, None, 3, 10, 0, 1, '6', '900', 2, None, None, 1, 2, 1, 70, 30, 11, 2019, '01:30', 4, '93', '93053', 1, 1, 1, 2, 'AUTOROUTE A3', '48,8962100', '2,4701200'), (201900000001, '138\\xa0306\\xa0524', 2, 2, 4, 2, 2002, 0, 1, 0, -1, -1, ' -1', -1, 2, 7, 0, 2, 5, 23, 1, None, 1, '3', 0.0, None, 3, 10, 0, 1, '6', '900', 2, None, None, 1, 2, 1, 70, 30, 11, 2019, '01:30', 4, '93', '93053', 1, 1, 1, 2, 'AUTOROUTE A3', '48,8962100', '2,4701200')]\n"
     ]
    }
   ],
   "source": [
    "# fetch the table data after update, quick check with the first 2 records\n",
    "print(\"Table data after updation :\")\n",
    "cur.execute(\"SELECT * FROM df_sql LIMIT 2\") \n",
    "print(cur.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commit the changes and close the database \n",
    "con.commit()\n",
    "cur.close()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Back to pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we are going to pull the data from the database and use pandas dataframe to mainpulate it <br>\n",
    "We will leverage the power of pandas to perform the EDA and features engineering. <br>\n",
    ">From now on, `intermediate CSV files` will be saved at each milestone to store the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new df created from the master table (containing information from 4 datasets x 3 years) = 360k + records\n",
    "\n",
    "#creating connection to the database\n",
    "con = sqlite3.connect(\"db/capstone_sqlite.db\") \n",
    "\n",
    "# from this database, target the table containing our aggregated 360k records and import that into a dataframe\n",
    "df_from_sql = pd.read_sql_query('select * from df_sql', con) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(367261, 53)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_sql.shape # this ties back with our expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'XNVD3YBW5QSR05DK',\n",
       "  'HostId': 'BWCL9LbYg5GPxpykIicUePJnLY6rNYU5npOLra3r1jxYD4PuN4fyanwf08GvjG0Mc4nunFgS3OQ=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'BWCL9LbYg5GPxpykIicUePJnLY6rNYU5npOLra3r1jxYD4PuN4fyanwf08GvjG0Mc4nunFgS3OQ=',\n",
       "   'x-amz-request-id': 'XNVD3YBW5QSR05DK',\n",
       "   'date': 'Tue, 06 Dec 2022 11:19:38 GMT',\n",
       "   'etag': '\"16aedcf29e04686c483186124bed6803\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 1},\n",
       " 'ETag': '\"16aedcf29e04686c483186124bed6803\"'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket = aws_creds.bucket # using the variable to load in AWS\n",
    "csv_buffer = StringIO()\n",
    "df_from_sql.to_csv(csv_buffer)\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object(bucket, 'df_from_sql.csv').put(Body=csv_buffer.getvalue()) # exporting the dataframe to a csv file in AWS S3 buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part2, we will translate the data from french to english, and map the classes from numeric to english<br>\n",
    "This will allow a meaningful EDA in Part3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25a04f5bcc4cce45f88f55048bde1d1871ccb10dc4647418ede9299c65043aaf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
